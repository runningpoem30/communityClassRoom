# AutoMaintainer AI

> **A production-grade autonomous agent system that continuously improves open-source repositories without human intervention.**

![System Architecture](https://img.shields.io/badge/Agent-Cline-blue) ![Orchestration](https://img.shields.io/badge/Orchestration-Kestra-green) ![Review](https://img.shields.io/badge/Review-CodeRabbit-purple) ![Evaluation](https://img.shields.io/badge/Evaluation-Oumi-orange)

---

## ğŸ¯ What is AutoMaintainer AI?

AutoMaintainer AI is **not a chatbot**. It's a real agentic software system that:

1. **Analyzes** a GitHub repository autonomously
2. **Selects** exactly ONE meaningful improvement task per run
3. **Implements** the change using an autonomous coding agent (Cline)
4. **Opens** a pull request with clear reasoning
5. **Reviews** the PR using AI code review (CodeRabbit)
6. **Evaluates** quality with custom metrics (Oumi framework)
7. **Learns** from feedback to improve future runs
8. **Visualizes** its evolution over time on a live dashboard

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kestra    â”‚  â† Orchestrates entire lifecycle
â”‚  (Workflow) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â–º Clone Repo
       â”œâ”€â”€â–º Analyze Context (issues, TODOs, structure)
       â”‚
       â”œâ”€â”€â–º ğŸ§  Cline Agent
       â”‚    â”œâ”€ Select ONE task
       â”‚    â”œâ”€ Implement change
       â”‚    â””â”€ Write tests
       â”‚
       â”œâ”€â”€â–º Create Pull Request (GitHub)
       â”‚
       â”œâ”€â”€â–º ğŸ° CodeRabbit Review
       â”‚    â””â”€ Analyze code quality, risks, suggestions
       â”‚
       â”œâ”€â”€â–º ğŸ¯ Oumi Evaluation
       â”‚    â””â”€ Calculate Evolution Score (0-100)
       â”‚
       â”œâ”€â”€â–º ğŸ’¾ Store Results (PostgreSQL)
       â”‚    â””â”€ Learning memory for future runs
       â”‚
       â””â”€â”€â–º ğŸ“Š Update Dashboard (Next.js)
```

---

## ğŸ“ Project Structure

```
aiagentsassemble/
â”œâ”€â”€ agent/                    # Autonomous coding agent
â”‚   â”œâ”€â”€ prompts/             # Task selection, implementation, reflection
â”‚   â”œâ”€â”€ select_task.py       # LLM-based task selection
â”‚   â””â”€â”€ executor.py          # Cline integration
â”‚
â”œâ”€â”€ workflows/               # Kestra workflows
â”‚   â””â”€â”€ main-orchestration.yml  # Complete lifecycle orchestration
â”‚
â”œâ”€â”€ evaluation/             # Quality assessment
â”‚   â”œâ”€â”€ oumi_evaluator.py   # Custom evaluation metrics
â”‚   â””â”€â”€ coderabbit_integration.py
â”‚
â”œâ”€â”€ memory/                 # Persistent learning
â”‚   â”œâ”€â”€ schema.sql         # PostgreSQL database schema
â”‚   â”œâ”€â”€ memory_manager.py  # CRUD operations
â”‚   â””â”€â”€ retrieve_learnings.py
â”‚
â”œâ”€â”€ frontend/              # Next.js dashboard
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx      # Landing page with live stats
â”‚   â”‚   â”œâ”€â”€ timeline/     # Chronological run history
â”‚   â”‚   â”œâ”€â”€ analytics/    # Charts and trends
â”‚   â”‚   â””â”€â”€ api/data/     # API endpoint
â”‚   â””â”€â”€ lib/db.ts         # PostgreSQL connection
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup.sh          # One-command setup
â”‚
â”œâ”€â”€ docker-compose.yml    # PostgreSQL + Kestra
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Docker** (for PostgreSQL + Kestra)
- **Node.js 18+** (for frontend)
- **Python 3.11+** (for agent scripts)
- **API Keys**:
  - Anthropic (Claude) or OpenAI (GPT-4) for Cline
  - GitHub Personal Access Token
  - CodeRabbit API Key (optional)

### Setup (5 minutes)

```bash
# 1. Clone the repository
cd aiagentsassemble

# 2. Configure environment variables
cp .env.example .env
# Edit .env and add your API keys

# 3. Run setup script
chmod +x scripts/setup.sh
./scripts/setup.sh

# 4. Start the frontend
cd frontend
npm run dev
```

### Access Points

- **Dashboard**: http://localhost:3000
- **Kestra UI**: http://localhost:8080
- **API**: http://localhost:3000/api/data

---

## ğŸ® Running the System

### Option 1: Manual Trigger (Kestra UI)

1. Open http://localhost:8080
2. Navigate to **Flows** â†’ `automaintainer.main-orchestration`
3. Click **Execute**
4. Monitor progress in **Executions** tab
5. View results in dashboard at http://localhost:3000

### Option 2: Scheduled Runs

The workflow automatically runs daily at 2 AM (configurable in `workflows/main-orchestration.yml`).

### Option 3: API Trigger

```bash
curl -X POST http://localhost:8080/api/v1/executions/automaintainer/main-orchestration \
  -H "Content-Type: application/json" \
  -d '{"inputs": {"manual_trigger": true}}'
```

---

## ğŸ“Š Dashboard Features

### Landing Page
- **Live Stats**: Total runs, PRs created, merge rate, average score
- **System Architecture**: Visual component overview
- **Quick Navigation**: Timeline, Analytics, API access

### Timeline
- Chronological history of all agent runs
- Task details with type badges (bug fix, feature, refactor, etc.)
- Evolution scores with color coding (green â‰¥80, yellow â‰¥60, red <60)
- PR links and merge status
- CodeRabbit issue counts (security, performance, style)

### Analytics
- **Evolution Score Trend**: Line chart showing improvement over time
- **Task Type Distribution**: Pie chart of work categories
- **Key Metrics**: Success rate, average score, best/worst scores
- **Code Quality Trends**: Maintainability, readability, test coverage

---

## ğŸ§  Agent Intelligence

### Task Selection Process

The agent uses LLM reasoning to select tasks based on:

1. **Impact**: High value to maintainers/users
2. **Scope**: Completable in one PR
3. **Safety**: Low breaking-change risk
4. **Testability**: Can be verified
5. **Novelty**: Not duplicate work

**Selection Sources**:
- Open GitHub issues
-TODO comments in code
- Past PR learnings (what worked, what didn't)
- Code quality analysis

### Learning Loop

Every PR generates learnings stored in PostgreSQL:

```sql
learnings
â”œâ”€â”€ success_patterns  (repeat these)
â”œâ”€â”€ mistakes          (avoid these)
â”œâ”€â”€ best_practices    (from high-scoring PRs)
â””â”€â”€ anti_patterns     (from low-scoring PRs)
```

Future runs retrieve these learnings as context, enabling **continuous improvement**.

---

## ğŸ¯ Evaluation System (Oumi Framework)

Each PR receives an **Evolution Score** (0-100) based on:

| Metric | Weight | Measures |
|--------|--------|----------|
| **Code Quality** | 30% | Complexity, naming, structure |
| **Test Coverage** | 25% | Tests added, edge cases |
| **Maintainability** | 25% | Long-term impact, tech debt |
| **Readability** | 20% | Documentation, clarity |

**Additional Factors**:
- CodeRabbit review flags (security, performance, style)
- LLM qualitative assessment
- Historical performance

---

## ğŸ”§ Configuration

### Environment Variables

```bash
# LLM for agent
ANTHROPIC_API_KEY=sk-ant-...
CLINE_MODEL=claude-3-5-sonnet-20241022

# GitHub integration
GITHUB_TOKEN=ghp_...
GITHUB_REPO_OWNER=your-username
GITHUB_TARGET_REPO=demo-repo

# Database
DATABASE_URL=postgresql://postgres:password@localhost:5432/automaintainer

# Code review
CODERABBIT_API_KEY=your-coderabbit-key

# Evaluation
OUMI_API_KEY=sk-...  # OpenAI for evaluation LLM
OUMI_MODEL=gpt-4
```

### Workflow Customization

Edit `workflows/main-orchestration.yml` to:
- Change schedule (cron expression)
- Adjust timeouts
- Add custom steps
- Modify retry logic

---

## ğŸ“ˆ Demo Flow

**For Judges**: Here's a complete walkthrough:

1. **Setup** (5 min): Run `./scripts/setup.sh`
2. **Configure**: Add API keys to `.env`
3. **Start Services**: `docker-compose up -d`
4. **Start Frontend**: `cd frontend && npm run dev`
5. **Trigger Run**: Execute workflow in Kestra UI (http://localhost:8080)
6. **Monitor**: Watch Kestra execution logs
7. **View Results**: Check dashboard (http://localhost:3000)

**Expected Timeline** (per run):
- Clone & analyze: 1-2 min
- Task selection: 30 sec
- Implementation: 5-15 min (depends on task)
- PR creation: 10 sec
- CodeRabbit review: 2-5 min
- Evaluation: 30 sec
- **Total**: ~10-25 minutes per improvement

---

## ğŸ† Key Differentiators

### Why This is Production-Grade

âœ… **Observable**: Every step logged and traceable in Kestra
âœ… **Retry-Safe**: Idempotent workflows with error handling
âœ… **Persistent Memory**: PostgreSQL for audit trail and learning
âœ… **Real PRs**: Not mocked - actual GitHub pull requests
âœ… **Feedback Loop**: CodeRabbit + Oumi â†’ Learning â†’ Better future runs
âœ… **Scalable**: Docker + Kestra handles high volume
âœ… **Judge-Ready**: Clean frontend, clear architecture, real demo

### Not a Toy Demo

- âŒ No hardcoded data
- âŒ No fake PRs
- âŒ No manual intervention required
- âœ… Truly autonomous operation
- âœ… Real learning and improvement
- âœ… Production-ready infrastructure

---

## ğŸ› ï¸ Development

### Running Tests

```bash
# Backend tests
cd memory && pytest tests/
cd ../evaluation && pytest tests/
cd ../agent && pytest tests/

# Frontend tests
cd frontend && npm test
```

### Adding Custom Evaluation Metrics

Edit `evaluation/oumi_evaluator.py` and add your metric function:

```python
def evaluate_custom_metric(pr_data: Dict) -> float:
    # Your logic here
    return score  # 0-100
```

### Extending the Learning Loop

Add custom learning types in `memory/schema.sql`:

```sql
INSERT INTO learnings (pr_id, learning_type, summary, importance_score)
VALUES (%s, 'custom_pattern', %s, 0.8);
```

---

## ğŸ“œ Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Agent** | Cline (Claude/GPT-4) | Autonomous code implementation |
| **Orchestration** | Kestra | Workflow scheduling & coordination |
| **Code Review** | CodeRabbit | AI-powered PR review |
| **Evaluation** | Custom Oumi | Code quality scoring |
| **Database** | PostgreSQL | Memory & results storage |
| **Frontend** | Next.js 14 + TypeScript | Dashboard UI |
| **Styling** | Tailwind CSS | Modern, responsive design |
| **Charts** | Recharts | Analytics visualizations |
| **Deployment** | Docker Compose | Local orchestration |

---

## ğŸ› Troubleshooting

### Database Connection Errors

```bash
# Check if PostgreSQL is running
docker-compose ps

# Restart services
docker-compose restart
```

### Kestra Workflow Not Found

```bash
# Upload workflow manually
curl -X POST http://localhost:8080/api/v1/flows \
  -H "Content-Type: application/x-yaml" \
  --data-binary @workflows/main-orchestration.yml
```

### Frontend API Errors

- Ensure `DATABASE_URL` in `frontend/.env.local` matches Docker setup
- Check PostgreSQL logs: `docker-compose logs postgres`

---

## ğŸ“ License

MIT License - See [LICENSE](LICENSE) file

---

## ğŸ‘¥ Credits

Built for the **AI Agents Hackathon** by demonstrating production-grade autonomous systems.

**Technologies**:
- [Cline](https://github.com/cline/cline) - Autonomous coding agent
- [Kestra](https://kestra.io) - Workflow orchestration
- [CodeRabbit](https://coderabbit.ai) - AI code review
- [Next.js](https://nextjs.org) - React framework
- [PostgreSQL](https://postgresql.org) - Database

---

## ğŸš€ Future Enhancements

- [ ] Multi-repository support
- [ ] Slack/Discord notifications
- [ ] A/B testing different LLM models
- [ ] Advanced learning algorithms (RL)
- [ ] Cost tracking and optimization
- [ ] Self-healing on test failures
- [ ] Deployment to production (Vercel + cloud DB)

---

**Built with precision. Engineered for judges.**

For questions or demo requests, see the documentation in `/docs`.
